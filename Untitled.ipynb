{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10de5c7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T15:57:02.000936400Z",
     "start_time": "2024-01-16T15:57:00.755994700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 21:27:00.908541: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-16 21:27:00.908593: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-16 21:27:00.909189: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-16 21:27:00.913975: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-16 21:27:01.489999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f944c498",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T15:57:02.057034100Z",
     "start_time": "2024-01-16T15:57:02.052038Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 21:27:02.022263: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-16 21:27:02.047626: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-16 21:27:02.047687: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tf. config. list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f9c564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T15:57:02.059033500Z",
     "start_time": "2024-01-16T15:57:02.057034100Z"
    }
   },
   "outputs": [],
   "source": [
    "img_list = []\n",
    "resized_list = []\n",
    "for filename in glob.glob(\"/Users/RUDRA/project ntcc/cassava-leaf-diseases/Whitefly/*.jpg\"):\n",
    "    print(filename)\n",
    "    img = Image.open(filename)\n",
    "    img_list.append(img)\n",
    "for image in img_list:\n",
    "    image = image.resize((500,500))\n",
    "    resized_list.append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d839107b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T15:57:02.071569200Z",
     "start_time": "2024-01-16T15:57:02.060539400Z"
    }
   },
   "outputs": [],
   "source": [
    "for (i, new) in enumerate(resized_list):\n",
    "    new.save('{}{}{}'.format(\"/Users/RUDRA/project ntcc/newcassava-leaf-diseases/new whitefly/\", i+1, '.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39915e4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T15:57:02.072576600Z",
     "start_time": "2024-01-16T15:57:02.065055400Z"
    }
   },
   "outputs": [],
   "source": [
    "s_img = 300\n",
    "s_batch = 8\n",
    "channels = 3\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "699b447f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T15:57:02.220662500Z",
     "start_time": "2024-01-16T15:57:02.074575100Z"
    }
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory newcassava-leaf-diseases",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotFoundError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocessing\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage_dataset_from_directory\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnewcassava-leaf-diseases\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43ms_img\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ms_img\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43ms_batch\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/image_dataset.py:213\u001B[0m, in \u001B[0;36mimage_dataset_from_directory\u001B[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001B[0m\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m seed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    212\u001B[0m     seed \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m1e6\u001B[39m)\n\u001B[0;32m--> 213\u001B[0m image_paths, labels, class_names \u001B[38;5;241m=\u001B[39m \u001B[43mdataset_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex_directory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdirectory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    216\u001B[0m \u001B[43m    \u001B[49m\u001B[43mformats\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mALLOWLIST_FORMATS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclass_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_links\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_links\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m label_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(class_names) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m    224\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    225\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWhen passing `label_mode=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`, there must be exactly 2 \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    226\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass_names. Received: class_names=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclass_names\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    227\u001B[0m     )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/dataset_utils.py:542\u001B[0m, in \u001B[0;36mindex_directory\u001B[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001B[0m\n\u001B[1;32m    540\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    541\u001B[0m     subdirs \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 542\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m subdir \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msorted\u001B[39m(\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdirectory\u001B[49m\u001B[43m)\u001B[49m):\n\u001B[1;32m    543\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mio\u001B[38;5;241m.\u001B[39mgfile\u001B[38;5;241m.\u001B[39misdir(tf\u001B[38;5;241m.\u001B[39mio\u001B[38;5;241m.\u001B[39mgfile\u001B[38;5;241m.\u001B[39mjoin(directory, subdir)):\n\u001B[1;32m    544\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m subdir\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py:768\u001B[0m, in \u001B[0;36mlist_directory_v2\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    753\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001B[39;00m\n\u001B[1;32m    754\u001B[0m \n\u001B[1;32m    755\u001B[0m \u001B[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    765\u001B[0m \u001B[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001B[39;00m\n\u001B[1;32m    766\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_directory(path):\n\u001B[0;32m--> 768\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mNotFoundError(\n\u001B[1;32m    769\u001B[0m       node_def\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    770\u001B[0m       op\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    771\u001B[0m       message\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not find directory \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(path))\n\u001B[1;32m    773\u001B[0m \u001B[38;5;66;03m# Convert each element to string, since the return values of the\u001B[39;00m\n\u001B[1;32m    774\u001B[0m \u001B[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001B[39;00m\n\u001B[1;32m    775\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m    776\u001B[0m     compat\u001B[38;5;241m.\u001B[39mas_str_any(filename)\n\u001B[1;32m    777\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m _pywrap_file_io\u001B[38;5;241m.\u001B[39mGetChildren(compat\u001B[38;5;241m.\u001B[39mpath_to_bytes(path))\n\u001B[1;32m    778\u001B[0m ]\n",
      "\u001B[0;31mNotFoundError\u001B[0m: Could not find directory newcassava-leaf-diseases"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\"newcassava-leaf-diseases\",\n",
    "    shuffle = True,\n",
    "    image_size = (s_img, s_img),\n",
    "    batch_size = s_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f5cde7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T15:57:02.227182800Z",
     "start_time": "2024-01-16T15:57:02.222668900Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = dataset.class_names\n",
    "classes\n",
    "len(dataset)\n",
    "for image_batch, label_batch in dataset.take(1):\n",
    "    plt.imshow(image_batch[0].numpy().astype(\"uint8\"))\n",
    "    plt.title(classes[label_batch[0]])\n",
    "    print(image_batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4475c3fa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T15:57:02.224667500Z"
    }
   },
   "outputs": [],
   "source": [
    "#80% = training\n",
    "#20% = 10% validation, 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff2da8b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T15:57:02.226241400Z"
    }
   },
   "outputs": [],
   "source": [
    "#function for segments for train, test and val\n",
    "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    ds_size = len(ds)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d6a10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T15:57:02.228180100Z",
     "start_time": "2024-01-16T15:57:02.228180100Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b747857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T15:57:02.229685400Z",
     "start_time": "2024-01-16T15:57:02.228180100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "epochs = 20\n",
    "s_img = 300\n",
    "channels = 3\n",
    "classes_num = 6\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(s_img, s_img, channels)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(classes_num, activation='softmax')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9fa8ec",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T15:57:02.230204600Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df256f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T15:57:02.232212100Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', \n",
    "             loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:57:02.234213Z",
     "start_time": "2024-01-16T15:57:02.233212300Z"
    }
   },
   "id": "c8f3c342c2c15970",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfa0fa2",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2024-01-16T15:57:02.235212800Z"
    }
   },
   "outputs": [],
   "source": [
    "We_go = model.fit(train_ds, epochs = epochs, batch_size = s_batch, verbose = 1, validation_data = val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9620359d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T15:57:02.237213500Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_acc = model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3448e19a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T15:57:02.238213600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the testing/validation data\n",
    "predictions = model.predict(test_ds)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(classes_num, axis=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion_mat = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac6717",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T15:57:02.240730Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf30103",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T15:57:02.241727800Z"
    }
   },
   "outputs": [],
   "source": [
    "We_go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cca269",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T15:57:02.242727600Z"
    }
   },
   "outputs": [],
   "source": [
    "We_go.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9166b6c0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T15:57:02.243769Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = We_go.history['loss']\n",
    "val_loss = We_go.history['val_loss']\n",
    "accuracy = We_go.history['accuracy']\n",
    "val_accuracy = We_go.history['val_accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad8cbb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T15:57:02.243769Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(epochs), accuracy, label = 'Training Accuracy')\n",
    "plt.plot(range(epochs), val_accuracy, label = 'Validation Accuracy')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.title('Accuracy plot')\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(epochs), loss, label = 'Training Loss')\n",
    "plt.plot(range(epochs), val_loss, label = 'Validation Loss')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff2c88",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2024-01-16T15:57:02.247290800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras import models\n",
    "\n",
    "# Convert the TensorFlow Dataset to NumPy arrays\n",
    "train_images_np = []\n",
    "train_labels_np = []\n",
    "\n",
    "for image, label in train_ds:\n",
    "    train_images_np.append(image.numpy())\n",
    "    train_labels_np.append(label.numpy())\n",
    "\n",
    "train_images_np = np.concatenate(train_images_np, axis=0)\n",
    "train_labels_np = np.concatenate(train_labels_np, axis=0)\n",
    "\n",
    "# Train your CNN model and obtain the predicted probabilities or class labels for the training data\n",
    "cnn_predictions_train = model.predict(train_images_np)\n",
    "\n",
    "# Flatten the CNN predictions to use as input features for the Random Forest\n",
    "cnn_predictions_train_flattened = cnn_predictions_train.reshape((len(train_images_np), -1))\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the Random Forest using the CNN predictions as features and true labels as target variable\n",
    "random_forest.fit(cnn_predictions_train_flattened, train_labels_np)\n",
    "\n",
    "# Obtain the predicted probabilities or class labels from the CNN for the validation/test data\n",
    "cnn_predictions_val = model.predict(val_ds)\n",
    "cnn_predictions_val_flattened = cnn_predictions_val.reshape((len(val_ds ), -1))\n",
    "\n",
    "# Use the CNN predictions as features to make predictions with the Random Forest\n",
    "rf_predictions_val = random_forest.predict(cnn_predictions_val_flattened)\n",
    "\n",
    "# Evaluate the performance of the combined model\n",
    "acc = np.mean(rf_predictions_val == val_labels)\n",
    "print(\"Validation Accuracy:\", acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d1ba3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T15:57:02.247290800Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "def create_vgg16_model_with_multi_head_attention(num_classes, num_heads=4):\n",
    "    # Load the VGG16 model without the top (fully connected) layers\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "    \n",
    "    # Freeze the pretrained layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Define the input tensor\n",
    "    inputs = Input(shape=(128,128,3))\n",
    "    \n",
    "    # Pass the input through the VGG16 base model\n",
    "    vgg_output = base_model(inputs)\n",
    "    \n",
    "    # Apply multi-head attention on the output of VGG16\n",
    "    attention_outputs = []\n",
    "    for _ in range(num_heads):\n",
    "        attention_outputs.append(tf.keras.layers.Attention()([vgg_output, vgg_output]))\n",
    "    \n",
    "    # Concatenate the attention outputs\n",
    "    attention_concat = tf.keras.layers.Concatenate()(attention_outputs)\n",
    "    \n",
    "    # Flatten the output\n",
    "    flatten = Flatten()(attention_concat)\n",
    "    \n",
    "    # Fully connected layers\n",
    "    fc1 = Dense(4096, activation='relu')(flatten)\n",
    "    fc1_dropout = Dropout(0.5)(fc1)\n",
    "    fc2 = Dense(4096, activation='relu')(fc1_dropout)\n",
    "    fc2_dropout = Dropout(0.5)(fc2)\n",
    "    predictions = Dense(num_classes, activation='softmax')(fc2_dropout)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage example\n",
    "num_classes = 3\n",
    "num_heads = 6\n",
    "model = create_vgg16_model_with_multi_head_attention(num_classes, num_heads)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
